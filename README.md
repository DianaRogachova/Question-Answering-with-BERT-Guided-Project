# Question-Answering-with-BERT-Guided-Project

As I learn more about Large Language Models (LLMs), I get more and more interested in its different applications. For this guided project, I watched this video titled Applying BERT to Question Answering by ChrisMcCormickAI, where the author demonstrates the application of Bidirectional Encoder Representations from Transformers (BERT) for question-answering. All the code and information in this notebook is taken from Chris's video and colab notebook linked in the video description. My aim here is to follow the steps and 'learn-by-doing' BERT's application in question-answering.

The data for this project comes The Standford Question-Answering Dataset or SQuAD. The model we will be working with has already been trained and fine-tuned on this data, therefore we do not need to load and train the model ourselves in this project. As Chris explains in the video, the reason for not fine-tuning the BERT model ourselves is because the model has already learned how to do the question-answering from SQuAD benchmark, and will do a good job regardless of the text given to it. For other tasks, such as text classification, it is however a good idea to fine-tune the model on your specific dataset.
